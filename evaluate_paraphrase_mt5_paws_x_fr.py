# -*- coding: utf-8 -*-
"""evaluate paraphrase mt5 paws-x-fr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1edpQoTXL89CGjO0nsJKTcF_rU7kmam9M
"""

# !pip install simpletransformers boto3 wandb --quiet

import torch
import numpy as np
import boto3
import os
import random
# from tqdm.notebook import tqdm
from tqdm.notebook import tqdm
import datasets
import wandb
import logging
import pandas as pd
from simpletransformers.t5 import T5Model, T5Args
import os
import pandas as pd

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False 
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
set_seed(7)
tqdm.pandas()

# For logging loss
wandb.login(key="c7deb1bb77ce9433eb246d460385f363659145a8")

# session = boto3.Session(
#     aws_access_key_id='AKIA4QB2WTN57SCTNAGG',
#     aws_secret_access_key='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',
# )
# s3 = session.resource('s3')
# s3.meta.client.download_file(Bucket='mtacl', Key='final_mt5_fr_en_test_prediction', Filename='final_mt5_fr_en_test_prediction.csv')

session = boto3.Session(
    aws_access_key_id='AKIA4QB2WTN57SCTNAGG',
    aws_secret_access_key='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',
)
s3 = session.resource('s3')
s3.meta.client.download_file(Bucket='mtacl', Key='paraphrase_input_fr', Filename='paraphrase_input_fr.csv')

def get_not_normal_text(input_text, target_text):
  ts_ratio = len(target_text)/len(input_text)
  prefix = "par normal"
  if ts_ratio < 0.95 or ts_ratio > 1.10:
    target_text = prefix + " " + target_text
  return target_text

# mt_prediction_fr = pd.read_csv('final_mt5_fr_en_test_prediction.csv')
# mt_prediction_fr['input_text'] = mt_prediction_fr.progress_apply(
#     lambda row: row['input_text'].replace('normal ', ''),
#     axis=1
# )
# mt_prediction_fr.to_csv('final_mt5_fr_en_test_prediction.csv', index=False)
# mt_prediction_fr['proc_pred'] = mt_prediction_fr.progress_apply(
#     lambda row: get_not_normal_text(row['input_text'], row['preds']),
#     axis=1
# )
# mt_prediction_fr.drop(['preds'], inplace=True, axis=1)
# mt_prediction_fr.rename(columns={
#     'proc_pred': 'mt_prediction'
# }, inplace=True)
# mt_prediction_fr.to_csv('paraphrase_input_fr.csv', index=False)

print("loading preprocessed test data")
test = pd.read_csv('paraphrase_input_fr.csv')
test['index'] = list(range(0, test.shape[0]))
test_non_normal = test.progress_apply(
    lambda row: row if row['mt_prediction'].startswith('par normal') else float('NaN'),
    axis=1
)
test_non_normal.dropna(inplace=True)
non_normal_index = test_non_normal['index'].astype(int).to_list()
test_normal = test[~test['index'].isin(non_normal_index)]

# output_list = []
# for instance in tqdm(paw_x_fr['train'], total=paw_x_fr['train'].shape[0]):
#   output_list.append(instance)

# output_df = pd.DataFrame(output_list)
# output_df.drop(['id', 'label'], axis=1, inplace=True)
# output_df.columns = ["input_text", "target_text"]
# output_df.to_csv("paws_fr_train", index=False)

# logging.basicConfig(level=logging.INFO)
# transformers_logger = logging.getLogger("transformers")
# transformers_logger.setLevel(logging.WARNING)


# train_df = train
# eval_df = validation

# train_df["prefix"] = ""
# eval_df["prefix"] = ""

# model_args = T5Args()
# # model_args.max_seq_length = 100
# model_args.train_batch_size = 2
# model_args.eval_batch_size = 2
# model_args.num_train_epochs = 2
# model_args.scheduler = "cosine_schedule_with_warmup"
# model_args.evaluate_during_training = True
# model_args.evaluate_during_training_steps = 10000
# model_args.learning_rate = 0.0003
# model_args.optimizer = 'Adafactor'
# model_args.use_multiprocessing = False
# model_args.fp16 = False
# model_args.save_steps = -1
# model_args.save_eval_checkpoints = False
# model_args.no_cache = True
# model_args.reprocess_input_data = True
# model_args.overwrite_output_dir = True
# model_args.save_model_every_epoch = True
# model_args.preprocess_inputs = False
# model_args.use_early_stopping = True
# model_args.num_return_sequences = 1
# model_args.do_lower_case = True
# model_args.output_dir = "all/"
# model_args.best_model_dir = "all/best_model"
# model_args.wandb_project = "prompt based MT5 fr paraphrase fine-tune 2"

# model = T5Model("mt5", "/content/checkpoint-7935-epoch-1/", args=model_args)
# #model.model.load_state_dict(torch.load("../input/semifinalyoruba/outputs/best_model/pytorch_model.bin"))

# # Train the model
# model.train_model(train_df, eval_data=eval_df)

# # Optional: Evaluate the model. We'll test it properly anyway.
# results = model.eval_model(eval_df, verbose=True)

# !unzip /content/mt5_checkpoitns_fr.zip
model_args = T5Args()
model_args.max_length = 100
model_args.length_penalty = 2.5
model_args.repetition_penalty = 1.5
model_args.num_beams = 5

print('instantiating model')
model = T5Model("mt5", "checkpoint-1459-epoch-1", args=model_args)

print('generating predictions')
preds = model.predict(test_non_normal.mt_prediction.values.tolist())
test_non_normal["mt_prediction"] = preds

# concat paraphrase and mt prediction
print('concatenating paraphrase and mt prediction dataframes')
test_new = pd.concat([test_normal, test_non_normal]).sort_values(by='index')
test_new['index'] = test_new['index'].astype(int)
test_new.to_csv('final_paraphrase_mt_prediction.csv', index=False)

print("saving final predictions to s3")
session = boto3.Session(
    aws_access_key_id='AKIA4QB2WTN57SCTNAGG',
    aws_secret_access_key='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',
)
s3 = session.resource('s3')
s3.meta.client.upload_file(Bucket='mtacl', Key='final_paraphrase_mt_prediction', Filename='final_paraphrase_mt_prediction.csv')