# -*- coding: utf-8 -*-
"""mt5 - test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZOhDhl1d1khVwCY5hzXwezx6ETGi2P2f
"""

# !pip install simpletransformers --quiet

# !pip install wandb --quiet

# !pip install boto3 --quiet

# from google.colab import drive
# drive.mount('/content/drive')

import torch
import numpy as np
import boto3
import os
import random
# from tqdm.notebook import tqdm
from tqdm import tqdm
import wandb
import logging
import pandas as pd
from simpletransformers.t5 import T5Model, T5Args
import os
import pandas as pd

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False 
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
set_seed(7)
tqdm.pandas()

#For logging loss
wandb.login(key="c7deb1bb77ce9433eb246d460385f363659145a8")

import boto3
session = boto3.Session(
    aws_access_key_id='AKIA4QB2WTN57SCTNAGG',
    aws_secret_access_key='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',
)
s3 = session.resource('s3')
s3.meta.client.download_file(Bucket='mtacl', Key='fr_en_train_data', Filename='fr_en_train.csv')

train = pd.read_csv('fr_en_train.csv')

#Remove any possible duplicates
train = train.drop_duplicates(subset=["en", "fr"])

train = train[["en", "fr"]]
train.columns = ["input_text", "target_text"]

def add_verbosity(input, target):
  ts_ratio = len(target.split(' '))/len(input.split(' '))
  if ts_ratio < 0.95:
    prefix = "short"
  elif ts_ratio >= 0.95 and ts_ratio <= 1.10:
    prefix = "normal"
  else:
    prefix = "long"
  return prefix + " " + input

train['input_text'] = train.progress_apply(
    lambda row: add_verbosity(row['input_text'], row['target_text']),
    axis=1
)

train

#Train 95% / Validation 5% Split
validation = train.sample(frac=0.05).astype(str)
train = train.drop(index=validation.index).astype(str)

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)


train_df = train
eval_df = validation

train_df["prefix"] = ""
eval_df["prefix"] = ""

model_args = T5Args()
model_args.max_seq_length = 100
model_args.train_batch_size = 10
model_args.eval_batch_size = 10
model_args.num_train_epochs = 8
model_args.scheduler = "cosine_schedule_with_warmup"
model_args.evaluate_during_training = True
model_args.evaluate_during_training_steps = 10000
model_args.learning_rate = 0.0003
model_args.optimizer = 'Adafactor'
model_args.use_multiprocessing = False
model_args.fp16 = False
model_args.save_steps = -1
model_args.save_eval_checkpoints = False
model_args.no_cache = True
model_args.reprocess_input_data = True
model_args.overwrite_output_dir = True
model_args.save_model_every_epoch = False
model_args.preprocess_inputs = False
model_args.use_early_stopping = True
model_args.num_return_sequences = 1
model_args.do_lower_case = True
model_args.output_dir = "all/"
model_args.best_model_dir = "all/best_model"
model_args.wandb_project = "verbosity based MT5 en-fr fine-tune"

model = T5Model("mt5", "google/mt5-base", args=model_args)
#model.model.load_state_dict(torch.load("../input/semifinalyoruba/outputs/best_model/pytorch_model.bin"))

# Train the model
model.train_model(train_df, eval_data=eval_df)

# Optional: Evaluate the model. We'll test it properly anyway.
results = model.eval_model(eval_df, verbose=True)

# model_args = T5Args()
# model_args.max_length = 100
# model_args.length_penalty = 2.5
# model_args.repetition_penalty = 1.5
# model_args.num_beams = 5

# model = T5Model("mt5", "all/best_model", args=model_args)

# preds = model.predict(validation.input_text.values.tolist())

# validation["preds"] = preds